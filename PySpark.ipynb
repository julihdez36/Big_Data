{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfk+/+TCsk6zoaa22S9OBo"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to PySpark\n",
        "\n",
        "Every pyspark program that you write will have these “core parts”, which are:\n",
        "\n",
        "1. importing the pyspark package (or modules)\n",
        "\n",
        "2. starting your Spark Session\n",
        "\n",
        "3. defining a set of transformations and actions over Spark DataFrames\n",
        "\n",
        "\n",
        "> PySpark is organized in a number of modules, such as sql (to access Spark SQL), pandas (to access the Pandas API of Spark), ml (to access Spark MLib).Going further, we can have sub-modules (or modules inside a module) too. As an example, the sql module of pyspark have the functions and window sub-modules.\n"
      ],
      "metadata": {
        "id": "tmQGlb5F7l7o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Starting your Spark Session\n",
        "\n",
        "You control your Spark application through a driver process called the SparkSession.\n",
        "\n",
        "The SparkSession instance is the way Spark executes user-defined manipulations across the clusters. There is a one-to-one correspondence between a SparkSession and Spark Applications.\n",
        "\n",
        "Every Spark application starts with a Spark Session. Basically, the Spark Session is the entry point to your application. This means that, in every pyspark program that you write, you should always start by defining your Spark Session. We do this, by using the getOrCreate() method from pyspark.sql.SparkSession.builder module.\n",
        "\n",
        "Just store the result of this method in any python object. Is very common to name this object as spark.\n"
      ],
      "metadata": {
        "id": "ZTHXszKHag0E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WF-w7pCa1vyh"
      },
      "outputs": [],
      "source": [
        "# Importar librería Spark e iniciar sesión\n",
        "# Toda aplicación de spark debe iniciar con una sesión (spark session)\n",
        "# guardarla como spark nos permitirá acceder a las funciones de la sesion\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.getOrCreate()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Definiendo conjunto de transformaciones y acciones\n",
        "# Todo programa spark está compuesto por un conjuntno de transformaciones y acciones sobre un conjunto de spark dataframes\n",
        "# Los cuales son la estructura básica que alimenta todos los programas de pyspark\n",
        "# Un programa de pyspark consiste en en transformar múltiples spark dataframes\n",
        "\n",
        "# Primera aplicación\n",
        "# Crear una tabla de una columna de 5 números, devolverá una lista simple\n",
        "\n",
        "df5 = spark.range(5) #método spark (almacena la secuencia como filas en una tabla spark)\n",
        "type(df5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 186
        },
        "id": "mroTGTyy4v6o",
        "outputId": "6f251e50-3862-426f-b952-9b5666d9b3db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pyspark.sql.dataframe.DataFrame"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>pyspark.sql.dataframe.DataFrame</b><br/>def __init__(jdf: JavaObject, sql_ctx: Union[&#x27;SQLContext&#x27;, &#x27;SparkSession&#x27;])</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py</a>A distributed collection of data grouped into named columns.\n",
              "\n",
              ".. versionadded:: 1.3.0\n",
              "\n",
              ".. versionchanged:: 3.4.0\n",
              "    Supports Spark Connect.\n",
              "\n",
              "Examples\n",
              "--------\n",
              "A :class:`DataFrame` is equivalent to a relational table in Spark SQL,\n",
              "and can be created using various functions in :class:`SparkSession`:\n",
              "\n",
              "&gt;&gt;&gt; people = spark.createDataFrame([\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 40, &quot;name&quot;: &quot;Hyukjin Kwon&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 50},\n",
              "...     {&quot;deptId&quot;: 1, &quot;age&quot;: 50, &quot;name&quot;: &quot;Takuya Ueshin&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 100},\n",
              "...     {&quot;deptId&quot;: 2, &quot;age&quot;: 60, &quot;name&quot;: &quot;Xinrong Meng&quot;, &quot;gender&quot;: &quot;F&quot;, &quot;salary&quot;: 150},\n",
              "...     {&quot;deptId&quot;: 3, &quot;age&quot;: 20, &quot;name&quot;: &quot;Haejoon Lee&quot;, &quot;gender&quot;: &quot;M&quot;, &quot;salary&quot;: 200}\n",
              "... ])\n",
              "\n",
              "Once created, it can be manipulated using the various domain-specific-language\n",
              "(DSL) functions defined in: :class:`DataFrame`, :class:`Column`.\n",
              "\n",
              "To select a column from the :class:`DataFrame`, use the apply method:\n",
              "\n",
              "&gt;&gt;&gt; age_col = people.age\n",
              "\n",
              "A more concrete example:\n",
              "\n",
              "&gt;&gt;&gt; # To create DataFrame using SparkSession\n",
              "... department = spark.createDataFrame([\n",
              "...     {&quot;id&quot;: 1, &quot;name&quot;: &quot;PySpark&quot;},\n",
              "...     {&quot;id&quot;: 2, &quot;name&quot;: &quot;ML&quot;},\n",
              "...     {&quot;id&quot;: 3, &quot;name&quot;: &quot;Spark SQL&quot;}\n",
              "... ])\n",
              "\n",
              "&gt;&gt;&gt; people.filter(people.age &gt; 30).join(\n",
              "...     department, people.deptId == department.id).groupBy(\n",
              "...     department.name, &quot;gender&quot;).agg({&quot;salary&quot;: &quot;avg&quot;, &quot;age&quot;: &quot;max&quot;}).show()\n",
              "+-------+------+-----------+--------+\n",
              "|   name|gender|avg(salary)|max(age)|\n",
              "+-------+------+-----------+--------+\n",
              "|     ML|     F|      150.0|      60|\n",
              "|PySpark|     M|       75.0|      50|\n",
              "+-------+------+-----------+--------+\n",
              "\n",
              "Notes\n",
              "-----\n",
              "A DataFrame should only be created as described above. It should not be directly\n",
              "created via using the constructor.</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 80);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r2QwsT936j26",
        "outputId": "f7a81092-b078-464b-e8d5-cde7c35d820a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df5.show() # Para ver los resultados"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ua2FM_Ac6miW",
        "outputId": "de0bc06f-fb16-47aa-d172-60a64fa0e89a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+\n",
            "| id|\n",
            "+---+\n",
            "|  0|\n",
            "|  1|\n",
            "|  2|\n",
            "|  3|\n",
            "|  4|\n",
            "+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lista5 = df5.collect() #mostramos los resultados en una lista de python\n",
        "lista5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_POu1n-6N8h",
        "outputId": "c0a6a727-5bfb-475d-e0d4-2ebb726826bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(id=0), Row(id=1), Row(id=2), Row(id=3), Row(id=4)]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos ver los diferentes métodos de los spark detaframe con dir()\n",
        "print(dir(df5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7hvqTCi6OGC",
        "outputId": "31bc3309-8717-4efb-db2e-dc6aa915f489"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattr__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_collect_as_arrow', '_ipython_key_completions_', '_jcols', '_jdf', '_jmap', '_joinAsOf', '_jseq', '_lazy_rdd', '_repr_html_', '_sc', '_schema', '_session', '_show_string', '_sort_cols', '_sql_ctx', '_support_repr_html', 'agg', 'alias', 'approxQuantile', 'cache', 'checkpoint', 'coalesce', 'colRegex', 'collect', 'columns', 'corr', 'count', 'cov', 'createGlobalTempView', 'createOrReplaceGlobalTempView', 'createOrReplaceTempView', 'createTempView', 'crossJoin', 'crosstab', 'cube', 'describe', 'distinct', 'drop', 'dropDuplicates', 'dropDuplicatesWithinWatermark', 'drop_duplicates', 'dropna', 'dtypes', 'exceptAll', 'explain', 'fillna', 'filter', 'first', 'foreach', 'foreachPartition', 'freqItems', 'groupBy', 'groupby', 'head', 'hint', 'id', 'inputFiles', 'intersect', 'intersectAll', 'isEmpty', 'isLocal', 'isStreaming', 'is_cached', 'join', 'limit', 'localCheckpoint', 'mapInArrow', 'mapInPandas', 'melt', 'na', 'observe', 'offset', 'orderBy', 'pandas_api', 'persist', 'printSchema', 'randomSplit', 'rdd', 'registerTempTable', 'repartition', 'repartitionByRange', 'replace', 'rollup', 'sameSemantics', 'sample', 'sampleBy', 'schema', 'select', 'selectExpr', 'semanticHash', 'show', 'sort', 'sortWithinPartitions', 'sparkSession', 'sql_ctx', 'stat', 'storageLevel', 'subtract', 'summary', 'tail', 'take', 'to', 'toDF', 'toJSON', 'toLocalIterator', 'toPandas', 'to_koalas', 'to_pandas_on_spark', 'transform', 'union', 'unionAll', 'unionByName', 'unpersist', 'unpivot', 'where', 'withColumn', 'withColumnRenamed', 'withColumns', 'withColumnsRenamed', 'withMetadata', 'withWatermark', 'write', 'writeStream', 'writeTo']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crando dataframe en spark\n",
        "\n",
        "# Primera forma: como colección de filas\n",
        "\n",
        "from datetime import date\n",
        "from pyspark.sql import Row\n",
        "\n",
        "data = [\n",
        "    Row(id = 1, value = 28.3, date = date(2021,1,1)),\n",
        "    Row(id = 2, value = 15.8, date = date(2021,1,2)),\n",
        "    Row(id = 3, value = 20.1, date = date(2021,1,3)),\n",
        "    Row(id = 4, value = 12.6, date = date(2021,1,4))\n",
        "]\n",
        "\n",
        "# Trasnformandolo a un df spark\n",
        "\n",
        "df = spark.createDataFrame(data)"
      ],
      "metadata": {
        "id": "eFpC9r9F6OL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dado que Spark es un objeto clase pyspark, imprimirlo es ver una descripción de las columnas presentes\n",
        "df\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uZ0Qo3d56OPg",
        "outputId": "0ab184d1-a80f-4dc0-e286-7a3aac02a92a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DataFrame[id: bigint, value: double, date: date]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Otra forma de crear un df spark es a través de listas\n",
        "\n",
        "# Creamos los registros\n",
        "data = [\n",
        "    [12114, 'Anne', 21, 1.56, 8, 9, 10, 9, 'Economics', 'SC'],\n",
        "    [13007, 'Adrian', 23, 1.82, 6, 6, 8, 7, 'Economics', 'SC'],\n",
        "    [10045, 'George', 29, 1.77, 10, 9, 10, 7, 'Law', 'SC'],\n",
        "    [12459, 'Adeline', 26, 1.61, 8, 6, 7, 7, 'Law', 'SC'],\n",
        "    [10190, 'Mayla', 22, 1.67, 7, 7, 7, 9, 'Design', 'AR'],\n",
        "    [11552, 'Daniel', 24, 1.75, 9, 9, 10, 9, 'Design', 'AR']\n",
        "]\n",
        "\n",
        "# Creamos los nombres de las columnas\n",
        "\n",
        "columns = [\n",
        "  'StudentID', 'Name', 'Age', 'Height', 'Score1',\n",
        "  'Score2', 'Score3', 'Score4', 'Course', 'Department'\n",
        "]\n",
        "\n",
        "students = spark.createDataFrame(data, columns)\n",
        "students.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nH0Kz5ub6OTp",
        "outputId": "69287cbc-e93a-4328-e8be-75c61ca7ac0d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-------+---+------+------+------+------+------+---------+----------+\n",
            "|StudentID|   Name|Age|Height|Score1|Score2|Score3|Score4|   Course|Department|\n",
            "+---------+-------+---+------+------+------+------+------+---------+----------+\n",
            "|    12114|   Anne| 21|  1.56|     8|     9|    10|     9|Economics|        SC|\n",
            "|    13007| Adrian| 23|  1.82|     6|     6|     8|     7|Economics|        SC|\n",
            "|    10045| George| 29|  1.77|    10|     9|    10|     7|      Law|        SC|\n",
            "|    12459|Adeline| 26|  1.61|     8|     6|     7|     7|      Law|        SC|\n",
            "|    10190|  Mayla| 22|  1.67|     7|     7|     7|     9|   Design|        AR|\n",
            "+---------+-------+---+------+------+------+------+------+---------+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Otros métodos para crear df spark se explorarán mas adelante\n",
        "\n",
        "# También podemos acceder al nombre de las comunas como una lista\n",
        "\n",
        "print(students.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nl2epRfk6OXp",
        "outputId": "2975752c-1050-4444-a2c9-0b399e21c690"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['StudentID', 'Name', 'Age', 'Height', 'Score1', 'Score2', 'Score3', 'Score4', 'Course', 'Department']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Para conocer el número de filas de mi df\n",
        "\n",
        "students.count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T2QtkWQH_M6J",
        "outputId": "a5587aa7-2536-4642-e663-668b28fa055a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spark admite múltiples tipos de datos, para mas información ir a: [enalce](https://spark.apache.org/docs/latest/sql-ref-datatypes.html)"
      ],
      "metadata": {
        "id": "2O48ouAU_a93"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Podemos ver informes mas completos del esquema del df y sus tipos\n",
        "\n",
        "df.printSchema()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IrgNiR0y_NAq",
        "outputId": "ff7a87b5-2dcb-4401-868c-65416b7ce519"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- id: long (nullable = true)\n",
            " |-- value: double (nullable = true)\n",
            " |-- date: date (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.schema # Nos da información con nombre, tipo y si puede contener nulos"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Utd6KTcw_ND7",
        "outputId": "ec3392e1-1e23-4f59-b337-f6af7aa706bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "StructType([StructField('id', LongType(), True), StructField('value', DoubleType(), True), StructField('date', DateType(), True)])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Si lo queremos ver por separado\n",
        "for i in df.schema:\n",
        "  print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cpcrUhyb_NHi",
        "outputId": "80386feb-b8a8-4402-baa4-841c4f7ea0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "StructField('id', LongType(), True)\n",
            "StructField('value', DoubleType(), True)\n",
            "StructField('date', DateType(), True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Si deseo acceder solamente a la información del tipado\n",
        "\n",
        "for i in df.schema:\n",
        "  print(i.dataType)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeRb4CDA_NKH",
        "outputId": "3d7578b6-0194-47ac-9be7-97610e778d3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LongType()\n",
            "DoubleType()\n",
            "DateType()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Lo mismo con el nombre y el valor booleano\n",
        "\n",
        "for i in df.schema:\n",
        "  print(i.name)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJ207Qn1_NNK",
        "outputId": "485737fc-6d70-4d89-9916-8633e8f00d47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "id\n",
            "value\n",
            "date\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creación de un esquema DF\n",
        "\n",
        "Cuando Spark crea un nuevo DataFrame, automáticamente adivinará qué esquema es apropiado para ese DataFrame. Pero a veces se equivoca mucho. En estos casos tienes que decirle a Spark exactamente cómo quieres que sea este esquema DataFrame."
      ],
      "metadata": {
        "id": "LP6QZWcOB5SZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField\n",
        "from pyspark.sql.types import DateType, StringType, IntegerType\n",
        "from datetime import date\n",
        "\n",
        "data = [\n",
        "    [1, date(2022,1,1), 'Anne'],\n",
        "    [2, date(2022,1,2), 'Layla'],\n",
        "    [3, date(2022,1,3), 'Wick'],\n",
        "    [4, date(2022,1,5), 'Paul']\n",
        "]\n",
        "\n",
        "schema = StructType([\n",
        "    StructField('ID', IntegerType(), True),\n",
        "    StructField('Date', DateType(), True),\n",
        "    StructField('Name', StringType(), True)\n",
        "])\n",
        "\n",
        "registers = spark.createDataFrame(data, schema = schema)\n"
      ],
      "metadata": {
        "id": "Gls7r5cn_NQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Para hacer verificación del tipo podemos hacerlo así\n",
        "# Preguntamos si es una instancia de una clase, en este caso clase integer\n",
        "\n",
        "isinstance(df.schema[0].dataType, IntegerType)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBA4oMeQ_NTZ",
        "outputId": "5e9fea71-e3b5-4795-91c4-980b10a51684"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# El resultado acá es falso porque en verdad se trata de un longtype\n",
        "# enteros con signo de 8 bytes\n",
        "\n",
        "from pyspark.sql.types import LongType\n",
        "isinstance(df.schema[0].dataType, LongType)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IT5lsJtLEziq",
        "outputId": "653313e5-c4d1-40b6-d696-e2d8c3fe7b60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Revisemos ahora la clase column\n",
        "# Notaremos que ya nos hemos familiarizado con ella\n",
        "\n",
        "print(type(df.id))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rzLXrLsKFcIV",
        "outputId": "2a974b1d-6316-41cc-ec23-58907fc7c8a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pyspark.sql.column.Column'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "id_column = col('ID')\n",
        "print(id_column)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdemDH7eFcMY",
        "outputId": "f4c7b528-45f3-42cb-81d3-f2698f209d34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column<'ID'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Column es sólo una expresión, no nos interesa por ahora el contenido o el df al que pertenece\n",
        "# Esto tienen que ver con el lazy aspect de Spark\n",
        "\n",
        "expr1 = id_column*2\n",
        "print(expr1)\n",
        "\n",
        "# Spark sabe lo que queremos hacer, pero no ejecuta aun la operación\n",
        "# Sólo lo ejecutara hasta que se solite la acción\n",
        "\n",
        "expr2 = (col('Name') == 'Anne') & (col('Gande') > 6)\n",
        "print(expr2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFsK0mRxFcPs",
        "outputId": "b3f5550a-e102-4bb8-9ad9-7770c3f7a5fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Column<'(ID * 2)'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Valores literales vs expresiones"
      ],
      "metadata": {
        "id": "AoUUkQonJWCg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1u9n8rwmFcSO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xABO_YIXFcVW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Z_tU6qb6FcxO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YpwC3MfDFc0j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UEhdt0OzFc3N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}